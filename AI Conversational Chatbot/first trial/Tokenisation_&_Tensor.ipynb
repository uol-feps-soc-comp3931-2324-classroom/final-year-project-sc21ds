{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "galKQBgRIgF8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "import json\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Function to tokenize and chunk text\n",
        "def tokenize_and_chunk(text):\n",
        "    # Split text into lines\n",
        "    lines = text.split('\\n')\n",
        "    # Remove the first 7 lines\n",
        "    lines = lines[7:]\n",
        "    # Rejoin the lines into a single string\n",
        "    text = '\\n'.join(lines)\n",
        "    # Tokenize sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    # Tokenize words and POS tagging for each sentence\n",
        "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "    tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]\n",
        "    # Perform named entity chunking\n",
        "    chunked_sentences = [ne_chunk(tagged_sentence) for tagged_sentence in tagged_sentences]\n",
        "    return chunked_sentences\n",
        "\n",
        "# Load JSON file\n",
        "try:\n",
        "    with open('/content/drive/MyDrive/articles_data (1).json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "    print(\"JSON file loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please make sure the file exists and try again.\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"Error decoding JSON file:\", e)\n",
        "    # Handle the error as needed\n",
        "\n",
        "# Tokenize and chunk each article and save to a new file\n",
        "if 'data' in locals():\n",
        "    with open('/content/drive/MyDrive/chunked_articles.json', 'w') as f:\n",
        "        for article in data:\n",
        "            article_id = article['Title']\n",
        "            article_text = article['Article Content']\n",
        "            chunked_article = tokenize_and_chunk(article_text)\n",
        "            f.write(f'Article ID: {article_id}\\n')\n",
        "            for sentence_tree in chunked_article:\n",
        "                f.write(str(sentence_tree) + '\\n')\n",
        "            f.write('\\n')\n",
        "else:\n",
        "    print(\"No data loaded due to errors. Please check the JSON file and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "id": "fqHf6MUBIkwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load tokenized and padded data\n",
        "json_file_path = '/content/drive/MyDrive/chunked_articles.json'\n",
        "model_save_path = '/content/drive/MyDrive/tensorflow_lstm_model'\n",
        "\n",
        "try:\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        data = f.read()\n",
        "        print(\"JSON file loaded successfully.\")\n",
        "        print(\"Data:\", data)  # Print the loaded data\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{json_file_path}' not found. Please make sure the file exists and try again.\")\n",
        "    exit()\n",
        "\n",
        "# Process the data\n",
        "article_contents = []\n",
        "article_content = []  # Initialize article content list\n",
        "for line in data:\n",
        "    if line.startswith(\"Article ID:\"):\n",
        "        # Start of a new article\n",
        "        if article_content:\n",
        "            article_contents.append(article_content)\n",
        "            article_content = []  # Reset article content for the new article\n",
        "    elif line.strip():  # Non-empty line\n",
        "        article_content.append(line.strip())  # Add tokenized/chunked content\n",
        "\n",
        "# Append the last article's content if present\n",
        "if article_content:\n",
        "    article_contents.append(article_content)\n",
        "\n",
        "# Define TensorFlow model using tf.keras API\n",
        "vocab_size = 10000  # Placeholder value\n",
        "embedding_dim = 128  # Placeholder value\n",
        "max_seq_length = 100  # Placeholder value\n",
        "hidden_size = 64  # Example hidden size\n",
        "num_classes = 2  # Example number of classes\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length),\n",
        "    tf.keras.layers.LSTM(units=hidden_size),\n",
        "    tf.keras.layers.Dense(units=num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Ensure the directory for saving the model exists\n",
        "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "model.save(model_save_path)\n"
      ],
      "metadata": {
        "id": "8rUbjSbjIsi8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}