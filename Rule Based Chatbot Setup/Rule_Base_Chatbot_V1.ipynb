{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "362e7836",
      "metadata": {
        "id": "362e7836"
      },
      "source": [
        "# Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "BvcjL_zkbm4j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvcjL_zkbm4j",
        "outputId": "f33b175b-f7da-45b0-d4fe-e35e6228b852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3f1ad633",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f1ad633",
        "outputId": "a5b3fabe-455d-4780-c106-5fff779c4037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "# apt install python3-contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f7e38af5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7e38af5",
        "outputId": "5a2c2e11-df4c-4121-aa6c-1b4d1b93c78c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ad6484b9",
      "metadata": {
        "id": "ad6484b9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import string\n",
        "import json\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import unicodedata\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import contractions\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb82381",
      "metadata": {
        "id": "0eb82381"
      },
      "source": [
        "# Read a dialog file and create a DataFrame of questions and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d875839d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "d875839d",
        "outputId": "2d71a0f1-86f2-4b5f-afd5-957e5b80a28c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 3945,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3819,\n        \"samples\": [\n          \"Why do many companies choose to employ children in the fashion industry?\",\n          \"How does Wuxly ensure ethical manufacturing of their jackets?\",\n          \"What is Stradivarius' stance on animal welfare?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3920,\n        \"samples\": [\n          \"PVC, or polyvinyl chloride, is a compound made by reacting chlorine, carbon, and ethylene together to create Vinyl Chloride Monomer, which is then polymerized to create base PVC. Additives are then included to shape the material into various forms, including clothing.\",\n          \"The founder of Big Sister Swap is Hudi Charin, who created the service after realizing the importance of sustainable fashion and the lack of time to attend clothes swap events or charity shops.\",\n          \"Good On You-rated favorites such as Article 22 and Outland Denim are included in the lineup for the ethical fashion pop up shops during New York Fashion Week.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-26a2aee7-fc17-41aa-aa03-51b83960d968\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Why is it important to support BIPOC owned fas...</td>\n",
              "      <td>Supporting BIPOC owned fashion brands is impor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is it challenging to find BIPOC owned fash...</td>\n",
              "      <td>It is challenging to find BIPOC owned fashion ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the significance of featuring BIPOC ow...</td>\n",
              "      <td>Featuring BIPOC owned brands in content is sig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How can supporting BIPOC owned brands contribu...</td>\n",
              "      <td>Supporting BIPOC owned brands is a tangible wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the message conveyed by the unique col...</td>\n",
              "      <td>The unique collections of BIPOC owned brands c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26a2aee7-fc17-41aa-aa03-51b83960d968')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-26a2aee7-fc17-41aa-aa03-51b83960d968 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-26a2aee7-fc17-41aa-aa03-51b83960d968');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-85af9a74-4262-4f03-bbd3-d7d6c021b388\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85af9a74-4262-4f03-bbd3-d7d6c021b388')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-85af9a74-4262-4f03-bbd3-d7d6c021b388 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  Why is it important to support BIPOC owned fas...   \n",
              "1  Why is it challenging to find BIPOC owned fash...   \n",
              "2  What is the significance of featuring BIPOC ow...   \n",
              "3  How can supporting BIPOC owned brands contribu...   \n",
              "4  What is the message conveyed by the unique col...   \n",
              "\n",
              "                                              answer  \n",
              "0  Supporting BIPOC owned fashion brands is impor...  \n",
              "1  It is challenging to find BIPOC owned fashion ...  \n",
              "2  Featuring BIPOC owned brands in content is sig...  \n",
              "3  Supporting BIPOC owned brands is a tangible wa...  \n",
              "4  The unique collections of BIPOC owned brands c...  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question  =[]\n",
        "answer = []\n",
        "# Opening JSON file\n",
        "f = open('/content/drive/MyDrive/data.json')\n",
        "\n",
        "data = json.load(f)\n",
        "\n",
        "i=0\n",
        "for i in range(len(data)):\n",
        "\n",
        "    for j in data[i][\"Q&As\"]:\n",
        "        question.append(list(j.values())[0])\n",
        "        answer.append(list(j.values())[1])\n",
        "data = pd.DataFrame({\"question\" : question ,\"answer\":answer})\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "40a89c92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40a89c92",
        "outputId": "ae8b4221-3105-4f4d-aef8-5a5d57731ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       Why is it important to support BIPOC owned fas...\n",
            "1       Why is it challenging to find BIPOC owned fash...\n",
            "2       What is the significance of featuring BIPOC ow...\n",
            "3       How can supporting BIPOC owned brands contribu...\n",
            "4       What is the message conveyed by the unique col...\n",
            "                              ...                        \n",
            "3940    What is the significance of voting with your d...\n",
            "3941    What inspired Rachel Temko to start Whimsy + Row?\n",
            "3942    How does Whimsy + Row reduce its carbon footpr...\n",
            "3943    What are some of the ethical practices followe...\n",
            "3944    How does Whimsy + Row contribute to the commun...\n",
            "Name: question, Length: 3945, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# print(data[0][\"Q&As\"][0][0])\n",
        "print(data[\"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "417587b5",
      "metadata": {
        "id": "417587b5"
      },
      "source": [
        "# Preprocessing text data using unicode normalization, regular expressions, and string manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "3cb7e88b",
      "metadata": {
        "id": "3cb7e88b"
      },
      "outputs": [],
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = unicode_to_ascii(text.lower().strip())\n",
        "    text = \" \".join(text.split())\n",
        "    text = ''.join([i for i in text if not i.isdigit()])\n",
        "    contractions.fix(text)\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"hi\", \"hey\", text)\n",
        "    text = re.sub(r\"\\r\", \"\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(\"(\\\\W)\",\" \",text)\n",
        "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
        "    text =  \"<sos> \" +  text + \" <eos>\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b378fbf",
      "metadata": {
        "id": "0b378fbf"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d8f3ae",
      "metadata": {
        "id": "70d8f3ae"
      },
      "source": [
        "# Clean, Convert, & Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "5085487d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5085487d",
        "outputId": "1f442d91-3847-49fa-86b5-d25fff5b27f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3945\n"
          ]
        }
      ],
      "source": [
        "data[\"question\"] = data.question.apply(clean_text)\n",
        "data[\"answer\"] = data.answer.apply(clean_text)\n",
        "question  = data.question.values.tolist()\n",
        "answer =  data.answer.values.tolist()\n",
        "\n",
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, lang_tokenizer\n",
        "print(len(data[\"question\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63795026",
      "metadata": {
        "id": "63795026"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f57929",
      "metadata": {
        "id": "39f57929"
      },
      "source": [
        "# Setting up data for sequence-to-sequence model training!\n",
        "`Here, we perform various data preprocessing steps like tokenization, removal of start and end tags, and creation of training and validation datasets using an 80-20 split. The tokenization process involves creating word-to-index and index-to-word dictionaries for both input and target languages. We can also set various hyperparameters for the model like the embedding dimension, number of units, vocabulary size, batch size, and buffer size.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "6e71c2b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e71c2b9",
        "outputId": "54cfa5c6-e658-43cf-c8c5-3644935a8509",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28 <keras.src.preprocessing.text.Tokenizer object at 0x78a33648baf0> [[   3  116 1702 ...    0    0    0]\n",
            " [   3   20    9 ...    0    0    0]\n",
            " [   3 1248 1702 ...    0    0    0]\n",
            " ...\n",
            " [   3  750  746 ...    0    0    0]\n",
            " [   3  750  746 ...    0    0    0]\n",
            " [   3  750  746 ...    0    0    0]] <keras.src.preprocessing.text.Tokenizer object at 0x78a072fde0e0>\n"
          ]
        }
      ],
      "source": [
        "input_tensor , inp_lang  =  tokenize(question)\n",
        "target_tensor , targ_lang  =  tokenize(answer)\n",
        "\n",
        "def remove_tags(sentence):\n",
        "    return sentence.split(\"<start>\")[-1].split(\"<end>\")[0]\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 100\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim =256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        "\n",
        "print((input_tensor[0][3]) , inp_lang ,target_tensor , targ_lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "81b2151e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "81b2151e",
        "outputId": "e66971ca-a061-4db0-fd10-424aaf73fe8c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<sos> consumers should ensure that the hemp fabric they are purchasing is organic and produced using environmentally friendly processes some companies use harmful chemicals in the production of hemp fabric so it is important to research the brand before making a purchase <eos>'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"answer\"][1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "76c9a491",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76c9a491",
        "outputId": "2ecf607d-3692-409f-d14e-4b0c22f7129f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  1  22   5  28  44   9  80 741 742  11  17   2   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "3945\n"
          ]
        }
      ],
      "source": [
        "# for i in target_tensor :\n",
        "#     print(i)\n",
        "print(input_tensor[0])\n",
        "print(len(target_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc5f273",
      "metadata": {
        "id": "5cc5f273"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d90fb8a4",
      "metadata": {
        "id": "d90fb8a4"
      },
      "source": [
        "# Encoder class for Sequence-to-Sequence model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b9f060ff",
      "metadata": {
        "id": "b9f060ff"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x,hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc8c15b",
      "metadata": {
        "id": "acc8c15b"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb4eba1",
      "metadata": {
        "id": "aeb4eba1"
      },
      "source": [
        "# Encoder Model Initialization and Output Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "dfc8c18c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfc8c18c",
        "outputId": "30424d14-fad5-473a-a234-4909636251d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (100, 30, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (100, 1024)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1a69e1d",
      "metadata": {
        "id": "f1a69e1d"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "566dced4",
      "metadata": {
        "id": "566dced4"
      },
      "source": [
        "# Bahdanau Attention Layer Implementation for Neural Machine Translation\n",
        "`The code defines a custom Keras layer that implements Bahdanau attention mechanism for Neural Machine Translation. Bahdanau attention is a type of attention mechanism that computes attention weights over the input sequence at every time step, and generates a context vector that is a weighted sum of the input sequence. This context vector is then used by the decoder to generate the output sequence. The implementation uses two dense layers, W1 and W2, to map the input and query to a common hidden space, and a dense layer V to generate the attention weights. The layer takes as input the query (decoder hidden state) and the values (encoder output sequence), and outputs the context vector and attention weights.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "40ecad4d",
      "metadata": {
        "id": "40ecad4d"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "#         print(\"attention_weights : \",attention_weights)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d61ce6",
      "metadata": {
        "id": "f5d61ce6"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b47f82",
      "metadata": {
        "id": "f9b47f82"
      },
      "source": [
        "# Applying Bahdanau Attention to Encoder Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "0619ac2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0619ac2d",
        "outputId": "36702812-f672-4260-990a-94a93fab39c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (100, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (100, 30, 1)\n"
          ]
        }
      ],
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc539491",
      "metadata": {
        "id": "cc539491"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc2eeba",
      "metadata": {
        "id": "cdc2eeba"
      },
      "source": [
        "# A decoder class that takes the output from the encoder and generates the target sequence word by word!\n",
        "`The Decoder class is a sub-class of the tf.keras.Model class, which defines the decoding part of the Sequence-to-Sequence model used for the chatbot. The decoder is responsible for generating the output sequence given the encoded input sequence and the previous state.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "e2650371",
      "metadata": {
        "id": "e2650371"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5317a7",
      "metadata": {
        "id": "7d5317a7"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf277fc",
      "metadata": {
        "id": "2bf277fc"
      },
      "source": [
        "# Decoder model initialization and sample output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "c4389eb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4389eb7",
        "outputId": "a8d5e0b1-0d88-412f-e7ce-6c51dc57362e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (100, 8650)\n"
          ]
        }
      ],
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad0fa87",
      "metadata": {
        "id": "4ad0fa87"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7724e2",
      "metadata": {
        "id": "bd7724e2"
      },
      "source": [
        "# Loss function using Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "dbab8b2c",
      "metadata": {
        "id": "dbab8b2c"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "#     print(\"real : \",real,\"\\npredicted : \",pred)\n",
        "#     print (\"loss_ : \",loss_)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edad26ce",
      "metadata": {
        "id": "edad26ce"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea9a0607",
      "metadata": {
        "id": "ea9a0607"
      },
      "source": [
        "# Training Step Function using Teacher Forcing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "8446a483",
      "metadata": {
        "id": "8446a483"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden,variables=np.array([]),gradients=np.array([])):#,variables=np.array([])\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "#             print(\"loss : \",loss)\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "#     if len(variables)<=0:\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "#     print(\"variables : \",variables)\n",
        "#     print(\"vrb : \",vrb)\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "#     print(\"gradients : \",gradients)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss, variables,gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b82284c",
      "metadata": {
        "id": "8b82284c"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd09dfa1",
      "metadata": {
        "id": "dd09dfa1"
      },
      "source": [
        "# Training loop for the Chatbot Model\n",
        "`It appears that the loss decreases with each epoch, which suggests that the model is improving and learning to generate more accurate translations. The decreasing loss values are also a good sign that the optimizer and loss function are appropriate for the task at hand.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d89234",
      "metadata": {
        "id": "f2d89234"
      },
      "outputs": [],
      "source": [
        "# from tempfile import TemporaryFile\n",
        "# outfile = TemporaryFile()\n",
        "# np.save(outfile,vrb[0].numpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439ce0d9",
      "metadata": {
        "id": "439ce0d9"
      },
      "outputs": [],
      "source": [
        "# if you are running first time then ignore running this cell\n",
        "vrb=grd=[]\n",
        "with open('/content/drive/MyDrive/vrb.npy', 'rb') as f:\n",
        "    ind=0\n",
        "    for i in f:\n",
        "        vrb.append(np.load(f,allow_pickle=True))\n",
        "        ind+=1\n",
        "with open('/content/drive/MyDrive/grd.npy', 'rb') as f:\n",
        "    ind=0\n",
        "    for i in f:\n",
        "        grd.append(np.load(f,allow_pickle=True))\n",
        "        ind+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "a67e6bcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a67e6bcf",
        "outputId": "c8e550a4-9367-409c-aee0-90b8a4f9300c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "grd\n",
            "\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "Epoch: 50 Loss:0.1190\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "vrb=grd=[]\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(epoch)\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss,vrb,grd = train_step(inp, targ, enc_hidden,vrb,grd)#,vrb,grd\n",
        "        total_loss += batch_loss\n",
        "\n",
        "with open('/content/drive/MyDrive/vrb.npy', 'wb') as f:\n",
        "    for i in range(len(vrb)):\n",
        "        np.save(f, vrb[i].numpy)\n",
        "    print(\"grd\\n\")\n",
        "with open('/content/drive/MyDrive/grd.npy', 'wb') as f:\n",
        "    for i in range(len(grd)):\n",
        "        print(i)\n",
        "        if(i==0 or i==4):\n",
        "            np.save(f,grd[i].indices.numpy())\n",
        "            np.save(f,grd[i].values.numpy())\n",
        "        else:\n",
        "            np.save(f,grd[i].numpy)\n",
        "#         np.save(f, )\n",
        "\n",
        "#     if(epoch % 2 == 0):\n",
        "    print('Epoch:{:3d} Loss:{:.4f}'.format(epoch,\n",
        "                                          total_loss / steps_per_epoch))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4832f9d5",
      "metadata": {
        "id": "4832f9d5"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35fed4ce",
      "metadata": {
        "id": "35fed4ce"
      },
      "source": [
        "# Evaluation function for the Chatbot Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "11a3889b",
      "metadata": {
        "id": "11a3889b"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "    sentence = clean_text(sentence)\n",
        "    try:\n",
        "        inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    except:\n",
        "        return \"I am sorry\",sentence\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<sos>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<eos>':\n",
        "            return remove_tags(result), remove_tags(sentence)\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return remove_tags(result), remove_tags(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faacc1b8",
      "metadata": {
        "id": "faacc1b8"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56dee762",
      "metadata": {
        "id": "56dee762"
      },
      "source": [
        "# Loading and using our Chatbot model to answer questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "cdd1cc3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "cdd1cc3e",
        "outputId": "bc24d18d-99e4-4537-b4ce-252fc4dbd5c8"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-19816581aaa9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted answer: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "def ask(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Question: %s' % (sentence))\n",
        "    print('Predicted answer: {}'.format(result))\n",
        "    return result\n",
        "ask(questions[36])\n",
        "print(\"Actual : \")\n",
        "print(questions[36],\"\\n\",answers[36])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ecf6a0",
      "metadata": {
        "id": "66ecf6a0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# #ask(questions[5])\n",
        "# print(questions[:11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6d60e326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "6d60e326",
        "outputId": "ed83d478-4a17-41c2-abb3-e20f6f738938",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: <sos> what is bipoc <eos>\n",
            "Predicted answer: chanels impact on the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry \n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'chanels impact on the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry is the unjustified and unsustainable speed in the fasheyon industry '"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ask('What is BIPOC?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5c8638c",
      "metadata": {
        "id": "a5c8638c"
      },
      "source": [
        "#"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
