{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7739073,"sourceType":"datasetVersion","datasetId":4519611}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install langchain langchain-community langchain-openai chromadb jq langchainhub","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:21:58.270740Z","iopub.execute_input":"2024-03-06T15:21:58.271058Z","iopub.status.idle":"2024-03-06T15:22:13.678708Z","shell.execute_reply.started":"2024-03-06T15:21:58.271035Z","shell.execute_reply":"2024-03-06T15:22:13.677272Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Using cached langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\nCollecting langchain-community\n  Using cached langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\nCollecting langchain-openai\n  Using cached langchain_openai-0.0.8-py3-none-any.whl.metadata (2.5 kB)\nCollecting chromadb\n  Using cached chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: jq in /opt/conda/lib/python3.10/site-packages (1.6.0)\nRequirement already satisfied: langchainhub in /opt/conda/lib/python3.10/site-packages (0.1.15)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-core<0.2,>=0.1.29 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.29)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.22)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: openai<2.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from langchain-openai) (1.13.3)\nRequirement already satisfied: tiktoken<1,>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from langchain-openai) (0.6.0)\nRequirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.1.1)\nRequirement already satisfied: chroma-hnswlib==0.7.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.7.3)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.9.0)\nRequirement already satisfied: pulsar-client>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.4.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.17.1)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Using cached opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.2)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.1)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.1.1)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.60.0)\nRequirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.2)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nRequirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (29.0.0)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.9.15)\nRequirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/conda/lib/python3.10/site-packages (from langchainhub) (2.31.0.20240218)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (23.2)\nRequirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.0.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.1.0)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (4.2.0)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Using cached opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Using cached opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Using cached opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Using cached opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nRequirement already satisfied: asgiref~=3.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Using cached opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nRequirement already satisfied: opentelemetry-instrumentation==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nRequirement already satisfied: opentelemetry-util-http==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\nUsing cached langchain-0.1.11-py3-none-any.whl (807 kB)\nUsing cached langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\nUsing cached langchain_openai-0.0.8-py3-none-any.whl (32 kB)\nUsing cached chromadb-0.4.24-py3-none-any.whl (525 kB)\nUsing cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nUsing cached opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nInstalling collected packages: opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-openai, langchain-community, langchain, chromadb\nSuccessfully installed chromadb-0.4.24 langchain-0.1.11 langchain-community-0.0.25 langchain-openai-0.0.8 langchain-text-splitters-0.0.1 opentelemetry-instrumentation-fastapi-0.43b0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ChatBot Without Memory","metadata":{}},{"cell_type":"markdown","source":"## Code to Initialize Chatbot","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.document_loaders import JSONLoader\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain import hub\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = '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'\n\ndef split_docs(documents,chunk_size=1000,chunk_overlap=100):\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    docs = text_splitter.split_documents(documents)\n    return docs\n\nembedding_function = OpenAIEmbeddings()\n\n\nloader = JSONLoader(file_path=\"/kaggle/input/data-articles-qa/data.json\", jq_schema=\".[]\", text_content=False)\n\ndocuments = loader.load()\n\ndocs = split_docs(documents)\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n\ndb = Chroma.from_documents(documents=docs, embedding=embedding_function)\n\nretriever = db.as_retriever()\n\nretriever_from_llm = MultiQueryRetriever.from_llm(\n    retriever=retriever, llm=llm\n)\n\n\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nrag_chain = (\n    {\"context\": retriever_from_llm | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:22:55.617740Z","iopub.execute_input":"2024-03-06T15:22:55.618041Z","iopub.status.idle":"2024-03-06T15:24:28.984228Z","shell.execute_reply.started":"2024-03-06T15:22:55.618019Z","shell.execute_reply":"2024-03-06T15:24:28.983235Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Validation and Feedback Loop","metadata":{}},{"cell_type":"code","source":"def validate_response(question, response, trusted_sources):\n    # Logic to validate the response\n    # For example, check if key facts in the response match those in trusted_sources\n    \n    prompt = ChatPromptTemplate.from_template(\"\"\" You are good at validating a response by comparing it\\\n    to the context provided. Validate the response by comparing it to the provided context and return 'True'\\\n    if it is valid otherwise return 'False'.\n    \n    context: {context}\n    \n    response: {response}\n    \n    \"\"\")\n    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    output_parser = StrOutputParser()\n    \n#     context = trusted_sources.get_relevant_documents(question)\n\n#     print(context[1])\n#     print()\n#     print(response)\n    \n    chain = {\"context\": retriever_from_llm | format_docs, \"response\": RunnablePassthrough()} | prompt | model | output_parser\n\n    result = chain.invoke(response)\n    \n#     print(result)\n    \n    \n    if \"True\" in result:\n        is_valid = True\n    elif \"False\" in result:\n        is_valid = False\n        \n    return is_valid\n","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:25:55.057608Z","iopub.execute_input":"2024-03-06T15:25:55.057951Z","iopub.status.idle":"2024-03-06T15:25:55.064899Z","shell.execute_reply.started":"2024-03-06T15:25:55.057926Z","shell.execute_reply":"2024-03-06T15:25:55.063873Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Code to send User query to ChatBot","metadata":{}},{"cell_type":"code","source":"query = \"What are the most sustainable fabric options available for clothing?\"\nanswer = rag_chain.invoke(query)\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:27:38.899339Z","iopub.execute_input":"2024-03-02T05:27:38.899911Z","iopub.status.idle":"2024-03-02T05:27:42.433299Z","shell.execute_reply.started":"2024-03-02T05:27:38.899870Z","shell.execute_reply":"2024-03-02T05:27:42.431445Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The most sustainable fabric options available for clothing include organic hemp, organic linen, recycled cotton, recycled wool, organic cotton, TENCEL, and Monocel. Lower-impact materials are recommended, such as recycled cotton, recycled wool, organic hemp, or organic linen. Choosing biodegradable fabrics like linen and avoiding synthetic materials like polyester can also contribute to sustainability in clothing choices.\n","output_type":"stream"}]},{"cell_type":"code","source":"Validation_result = validate_response(query,answer,retriever_from_llm)\n\nif Validation_result == False:\n    response = \"I'm not sure about that. Let me get more information and get back to you\"","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:41:25.819534Z","iopub.execute_input":"2024-03-02T05:41:25.820651Z","iopub.status.idle":"2024-03-02T05:41:30.899140Z","shell.execute_reply.started":"2024-03-02T05:41:25.820596Z","shell.execute_reply":"2024-03-02T05:41:30.897904Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"page_content='natural fibres. The claim that \\\\u201cnatural\\\\u201d fabrics are always best for the environment is questionable at best, as every fabric has its pros and cons, though there are of course better options.\\\\nBeyond that, there\\\\u2019s little chance that brands and shoppers are going to abandon synthetics anytime soon. For some products, like swimwear and rainproof outerwear, synthetic material is just way more practical and the best option we currently have.\\\\nSo what can we do to reduce microfibre pollution in the ocean and the air?\\\\nNow that we\\\\u2019ve covered the background, it\\\\u2019s time to get practical. Let\\\\u2019s look at what to do about microfibres in clothing on a case by case basis.\\\\nBuy less (new) stuff\\\\nThe number one way to reduce the environmental impact of our clothing choices is to buy less stuff, especially less new stuff. Consider spending (less) of your hard-earned dollars on second hand clothing to extend the life of fabrics already in existence.\\\\nCheck out The Five Rs' metadata={'seq_num': 34, 'source': '/kaggle/input/data-articles-qa/data.json'}\n\nThe most sustainable fabric options available for clothing include organic hemp, organic linen, recycled cotton, recycled wool, organic cotton, TENCEL, and Monocel. Lower-impact materials are recommended, such as recycled cotton, recycled wool, organic hemp, or organic linen. Choosing biodegradable fabrics like linen and avoiding synthetic materials like polyester can also contribute to sustainability in clothing choices.\nTrue\n(True, 'The most sustainable fabric options available for clothing include organic hemp, organic linen, recycled cotton, recycled wool, organic cotton, TENCEL, and Monocel. Lower-impact materials are recommended, such as recycled cotton, recycled wool, organic hemp, or organic linen. Choosing biodegradable fabrics like linen and avoiding synthetic materials like polyester can also contribute to sustainability in clothing choices.')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ChatBot with Memory Storage","metadata":{}},{"cell_type":"markdown","source":"## Code to Initialize Chatbot","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.document_loaders import JSONLoader\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain import hub\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_core.messages import AIMessage, HumanMessage\nimport os\n\n#Add OpenAI key\nos.environ[\"OPENAI_API_KEY\"] = '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'\n#Add your filepath\nfile_path = \"/kaggle/input/data-articles-qa/data.json\"\n\ndef split_docs(documents,chunk_size=1000,chunk_overlap=100):\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    docs = text_splitter.split_documents(documents)\n    return docs\n\nembedding_function = OpenAIEmbeddings()\n\n\nloader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n\ndocuments = loader.load()\n\ndocs = split_docs(documents)\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature = 0)\n\ndb = Chroma.from_documents(documents=docs, embedding=embedding_function)\n\nretriever = db.as_retriever()\n\nretriever_from_llm = MultiQueryRetriever.from_llm(\n    retriever=retriever, llm=llm\n)\n\n\nqa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\nUse the following pieces of retrieved context to answer the question. \\\nIf you don't know the answer, just say that you don't know. \\\nUse three sentences maximum and keep the answer concise.\\\n\n{context}\"\"\"\nqa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", qa_system_prompt),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\ncontextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\ncontextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\ndef contextualized_question(input: dict):\n    if input.get(\"chat_history\"):\n        return contextualize_q_chain\n    else:\n        return input[\"question\"]\n\n\nrag_chain = (\n    RunnablePassthrough.assign(\n        context=contextualized_question | retriever_from_llm | format_docs\n    )\n    | qa_prompt\n    | llm\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:47:58.001076Z","iopub.execute_input":"2024-03-02T05:47:58.001543Z","iopub.status.idle":"2024-03-02T05:50:10.669177Z","shell.execute_reply.started":"2024-03-02T05:47:58.001509Z","shell.execute_reply":"2024-03-02T05:50:10.667839Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Code to send User query to ChatBot","metadata":{}},{"cell_type":"code","source":"from langchain_core.messages import AIMessage, HumanMessage\nchat_history = []\n\nquery = \"What are the most sustainable fabric options available for clothing?\"\nai_msg = rag_chain.invoke({\"question\": query, \"chat_history\": chat_history})\n\n\nValidation_result = validate_response(query,ai_msg.content,retriever_from_llm)\n\nif Validation_result == False:\n    response = \"I'm not sure about that. Let me get more information and get back to you\"\nelse:\n    print(ai_msg.content)\n    chat_history.extend([HumanMessage(content=query), ai_msg])\n# print(chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:51:47.339376Z","iopub.execute_input":"2024-03-02T05:51:47.339931Z","iopub.status.idle":"2024-03-02T05:51:55.896131Z","shell.execute_reply.started":"2024-03-02T05:51:47.339891Z","shell.execute_reply":"2024-03-02T05:51:55.894613Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"The most sustainable fabric options for clothing include recycled cotton, recycled wool, organic hemp, and organic linen. These materials have lower environmental impacts compared to conventional options like conventional cotton or virgin polyester. Choosing biodegradable fabrics like linen can also contribute to sustainability in fashion.\n","output_type":"stream"}]},{"cell_type":"code","source":"second_question = \"can you mention some companies that use these materials\"\nai_msg2 = rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})\n\nValidation_result = validate_response(second_question,ai_msg2.content,retriever_from_llm)\n\nif Validation_result == False:\n    response = \"I'm not sure about that. Let me get more information and get back to you\"\n    \nelse:\n    print(ai_msg2.content)  ","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:52:02.342291Z","iopub.execute_input":"2024-03-02T05:52:02.342679Z","iopub.status.idle":"2024-03-02T05:52:13.926099Z","shell.execute_reply.started":"2024-03-02T05:52:02.342651Z","shell.execute_reply":"2024-03-02T05:52:13.924576Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Major brands like Adidas, ASOS, H&M, and Burberry have pledged to use 100% sustainable cotton by 2025. New technologies like blockchain are also being used to trace cotton supply chains and ensure ethical and sustainable practices. Additionally, brands like Afends, Mila.Vert, and Natasha Tonic are known for making sustainable hemp clothing that is on-trend and eco-friendly.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ChatBot Validation","metadata":{}},{"cell_type":"code","source":"import json\n","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:26:01.675774Z","iopub.execute_input":"2024-03-06T15:26:01.676179Z","iopub.status.idle":"2024-03-06T15:26:01.680522Z","shell.execute_reply.started":"2024-03-06T15:26:01.676149Z","shell.execute_reply":"2024-03-06T15:26:01.679711Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# def validate_response(question, response, trusted_sources):\n#     # Logic to validate the response\n#     # For example, check if key facts in the response match those in trusted_sources\n    \n#     prompt = ChatPromptTemplate.from_template(\"\"\" You are good at validating a response by comparing it\\\n#     to the context provided. Validate the response by comparing it to the provided context and return 'True'\\\n#     if it is valid otherwise return 'False'.\n    \n#     context: {context}\n    \n#     response: {response}\n    \n#     \"\"\")\n#     model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n#     output_parser = StrOutputParser()\n    \n# #     context = trusted_sources.get_relevant_documents(question)\n\n# #     print(context[1])\n# #     print()\n# #     print(response)\n    \n#     chain = {\"context\": retriever_from_llm | format_docs, \"response\": RunnablePassthrough()} | prompt | model | output_parser\n\n#     result = chain.invoke(response)\n    \n# #     print(result)\n    \n    \n#     if \"True\" in result:\n#         is_valid = True\n#     elif \"False\" in result:\n#         is_valid = False\n        \n#     return is_valid","metadata":{"execution":{"iopub.status.busy":"2024-03-05T18:36:17.253679Z","iopub.execute_input":"2024-03-05T18:36:17.254094Z","iopub.status.idle":"2024-03-05T18:36:17.261520Z","shell.execute_reply.started":"2024-03-05T18:36:17.254066Z","shell.execute_reply":"2024-03-05T18:36:17.260609Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def append_to_file(input_list):\n    # Open the file in append mode\n    with open(\"output.txt\", \"a\") as file:\n        # Iterate through the input list\n        for item in input_list:\n            # Convert each item to string and append to the file\n            file.write(str(item) + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:26:04.237131Z","iopub.execute_input":"2024-03-06T15:26:04.237640Z","iopub.status.idle":"2024-03-06T15:26:04.242565Z","shell.execute_reply.started":"2024-03-06T15:26:04.237619Z","shell.execute_reply":"2024-03-06T15:26:04.241449Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def ChatBot_Evaluation():\n    # Read existing JSON data from file\n    Result = []\n    \n    try:\n        with open('/kaggle/input/data-articles-qa/data.json', 'r') as file:\n            existing_data = json.load(file)\n    except FileNotFoundError:\n        # If the file doesn't exist, initialize with an empty list\n        print('error in file reading')\n        \n    for i in range(100,200):\n        \n        if 'Question' in existing_data[i]:\n            print(Question1)\n        \n        Temp_result = []\n        \n        article_content, Q_A = existing_data[i]['ArticleContent'], existing_data[i]['Q&As']\n#         print(article_content, Q_A)\n        if 'Question' in Q_A[0]:\n            for y in range(0,2):\n                query = Q_A[y]['Question']\n                answer = rag_chain.invoke(query)\n                #print(answer)\n            \n                Validation_result = validate_response(query,answer,article_content)\n\n                if Validation_result == False:\n                    print('False result')\n            \n                elif Validation_result == True:\n                    print('True result')\n                \n                Temp_result.append(Validation_result)\n\n        elif 'question' in Q_A[0]:\n            for z in range(0,2):\n                query = Q_A[z]['question']\n                answer = rag_chain.invoke(query)\n                #print(answer)\n            \n                Validation_result = validate_response(query,answer,article_content)\n\n                if Validation_result == False:\n                    print('False result')\n            \n                elif Validation_result == True:\n                    print('True result')\n                \n                Temp_result.append(Validation_result)\n            \n        append_to_file(Temp_result)\n        print('Result Added for index: ', i)\n        \n        Result.append(Temp_result)\n            \n            \n            \n    return Result\n                ","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:16:56.566390Z","iopub.execute_input":"2024-03-06T16:16:56.566710Z","iopub.status.idle":"2024-03-06T16:16:56.577094Z","shell.execute_reply.started":"2024-03-06T16:16:56.566666Z","shell.execute_reply":"2024-03-06T16:16:56.575989Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"Result = ChatBot_Evaluation()\n\n\nprint(' ')\n\nprint(Result)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:56:45.818639Z","iopub.execute_input":"2024-03-06T15:56:45.818973Z","iopub.status.idle":"2024-03-06T16:15:04.765726Z","shell.execute_reply.started":"2024-03-06T15:56:45.818950Z","shell.execute_reply":"2024-03-06T16:15:04.764490Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"True result\nTrue result\nResult Added for index:  36\nTrue result\nTrue result\nResult Added for index:  37\nTrue result\nTrue result\nResult Added for index:  38\nTrue result\nTrue result\nResult Added for index:  39\nFalse result\nTrue result\nResult Added for index:  40\nTrue result\nTrue result\nResult Added for index:  41\nTrue result\nTrue result\nResult Added for index:  42\nTrue result\nTrue result\nResult Added for index:  43\nTrue result\nTrue result\nResult Added for index:  44\nTrue result\nTrue result\nResult Added for index:  45\nTrue result\nTrue result\nResult Added for index:  46\nTrue result\nTrue result\nResult Added for index:  47\nTrue result\nTrue result\nResult Added for index:  48\nTrue result\nTrue result\nResult Added for index:  49\nTrue result\nTrue result\nResult Added for index:  50\nTrue result\nTrue result\nResult Added for index:  51\nTrue result\nTrue result\nResult Added for index:  52\nTrue result\nTrue result\nResult Added for index:  53\nTrue result\nTrue result\nResult Added for index:  54\nTrue result\nTrue result\nResult Added for index:  55\nTrue result\nTrue result\nResult Added for index:  56\nTrue result\nTrue result\nResult Added for index:  57\nTrue result\nTrue result\nResult Added for index:  58\nTrue result\nTrue result\nResult Added for index:  59\nTrue result\nTrue result\nResult Added for index:  60\nTrue result\nFalse result\nResult Added for index:  61\nTrue result\nTrue result\nResult Added for index:  62\nTrue result\nTrue result\nResult Added for index:  63\nTrue result\nTrue result\nResult Added for index:  64\nTrue result\nTrue result\nResult Added for index:  65\nTrue result\nTrue result\nResult Added for index:  66\nTrue result\nTrue result\nResult Added for index:  67\nTrue result\nTrue result\nResult Added for index:  68\nTrue result\nTrue result\nResult Added for index:  69\nTrue result\nTrue result\nResult Added for index:  70\nTrue result\nTrue result\nResult Added for index:  71\nTrue result\nTrue result\nResult Added for index:  72\nTrue result\nTrue result\nResult Added for index:  73\nTrue result\nTrue result\nResult Added for index:  74\nTrue result\nTrue result\nResult Added for index:  75\nTrue result\nTrue result\nResult Added for index:  76\nTrue result\nTrue result\nResult Added for index:  77\nTrue result\nTrue result\nResult Added for index:  78\nTrue result\nTrue result\nResult Added for index:  79\nTrue result\nTrue result\nResult Added for index:  80\nTrue result\nTrue result\nResult Added for index:  81\nTrue result\nTrue result\nResult Added for index:  82\nTrue result\nTrue result\nResult Added for index:  83\nTrue result\nTrue result\nResult Added for index:  84\nTrue result\nTrue result\nResult Added for index:  85\nTrue result\nTrue result\nResult Added for index:  86\nTrue result\nTrue result\nResult Added for index:  87\nFalse result\nTrue result\nResult Added for index:  88\nTrue result\nTrue result\nResult Added for index:  89\nTrue result\nTrue result\nResult Added for index:  90\nTrue result\nTrue result\nResult Added for index:  91\nTrue result\nTrue result\nResult Added for index:  92\nTrue result\nTrue result\nResult Added for index:  93\nTrue result\nTrue result\nResult Added for index:  94\nTrue result\nTrue result\nResult Added for index:  95\nTrue result\nTrue result\nResult Added for index:  96\nTrue result\nFalse result\nResult Added for index:  97\nTrue result\nTrue result\nResult Added for index:  98\nTrue result\nTrue result\nResult Added for index:  99\n \n[[True, True], [True, True], [True, True], [True, True], [False, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, False], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [False, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, False], [True, True], [True, True]]\n","output_type":"stream"}]},{"cell_type":"code","source":"Result2 = ChatBot_Evaluation()\n\n\nprint(' ')\n\nprint(Result)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:17:04.599498Z","iopub.execute_input":"2024-03-06T16:17:04.599855Z","iopub.status.idle":"2024-03-06T16:42:30.961919Z","shell.execute_reply.started":"2024-03-06T16:17:04.599829Z","shell.execute_reply":"2024-03-06T16:42:30.960488Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"True result\nTrue result\nResult Added for index:  100\nTrue result\nTrue result\nResult Added for index:  101\nTrue result\nTrue result\nResult Added for index:  102\nTrue result\nTrue result\nResult Added for index:  103\nTrue result\nTrue result\nResult Added for index:  104\nTrue result\nTrue result\nResult Added for index:  105\nTrue result\nTrue result\nResult Added for index:  106\nResult Added for index:  107\nTrue result\nTrue result\nResult Added for index:  108\nTrue result\nTrue result\nResult Added for index:  109\nTrue result\nTrue result\nResult Added for index:  110\nTrue result\nTrue result\nResult Added for index:  111\nTrue result\nTrue result\nResult Added for index:  112\nTrue result\nTrue result\nResult Added for index:  113\nTrue result\nTrue result\nResult Added for index:  114\nTrue result\nTrue result\nResult Added for index:  115\nTrue result\nTrue result\nResult Added for index:  116\nTrue result\nTrue result\nResult Added for index:  117\nTrue result\nTrue result\nResult Added for index:  118\nTrue result\nTrue result\nResult Added for index:  119\nTrue result\nTrue result\nResult Added for index:  120\nTrue result\nTrue result\nResult Added for index:  121\nTrue result\nTrue result\nResult Added for index:  122\nTrue result\nTrue result\nResult Added for index:  123\nTrue result\nTrue result\nResult Added for index:  124\nTrue result\nTrue result\nResult Added for index:  125\nTrue result\nTrue result\nResult Added for index:  126\nTrue result\nTrue result\nResult Added for index:  127\nTrue result\nTrue result\nResult Added for index:  128\nTrue result\nTrue result\nResult Added for index:  129\nTrue result\nTrue result\nResult Added for index:  130\nTrue result\nTrue result\nResult Added for index:  131\nTrue result\nTrue result\nResult Added for index:  132\nTrue result\nTrue result\nResult Added for index:  133\nTrue result\nTrue result\nResult Added for index:  134\nTrue result\nTrue result\nResult Added for index:  135\nTrue result\nTrue result\nResult Added for index:  136\nTrue result\nTrue result\nResult Added for index:  137\nTrue result\nTrue result\nResult Added for index:  138\nTrue result\nTrue result\nResult Added for index:  139\nTrue result\nTrue result\nResult Added for index:  140\nTrue result\nTrue result\nResult Added for index:  141\nTrue result\nTrue result\nResult Added for index:  142\nTrue result\nTrue result\nResult Added for index:  143\nFalse result\nTrue result\nResult Added for index:  144\nTrue result\nTrue result\nResult Added for index:  145\nTrue result\nTrue result\nResult Added for index:  146\nTrue result\nTrue result\nResult Added for index:  147\nTrue result\nTrue result\nResult Added for index:  148\nTrue result\nTrue result\nResult Added for index:  149\nTrue result\nTrue result\nResult Added for index:  150\nTrue result\nTrue result\nResult Added for index:  151\nTrue result\nTrue result\nResult Added for index:  152\nTrue result\nTrue result\nResult Added for index:  153\nTrue result\nTrue result\nResult Added for index:  154\nTrue result\nFalse result\nResult Added for index:  155\nTrue result\nTrue result\nResult Added for index:  156\nTrue result\nTrue result\nResult Added for index:  157\nTrue result\nTrue result\nResult Added for index:  158\nTrue result\nTrue result\nResult Added for index:  159\nTrue result\nTrue result\nResult Added for index:  160\nTrue result\nTrue result\nResult Added for index:  161\nTrue result\nTrue result\nResult Added for index:  162\nTrue result\nFalse result\nResult Added for index:  163\nTrue result\nTrue result\nResult Added for index:  164\nTrue result\nTrue result\nResult Added for index:  165\nTrue result\nTrue result\nResult Added for index:  166\nTrue result\nTrue result\nResult Added for index:  167\nTrue result\nTrue result\nResult Added for index:  168\nTrue result\nTrue result\nResult Added for index:  169\nTrue result\nTrue result\nResult Added for index:  170\nTrue result\nTrue result\nResult Added for index:  171\nFalse result\nTrue result\nResult Added for index:  172\nTrue result\nTrue result\nResult Added for index:  173\nTrue result\nTrue result\nResult Added for index:  174\nTrue result\nTrue result\nResult Added for index:  175\nFalse result\nTrue result\nResult Added for index:  176\nTrue result\nTrue result\nResult Added for index:  177\nTrue result\nTrue result\nResult Added for index:  178\nTrue result\nTrue result\nResult Added for index:  179\nTrue result\nTrue result\nResult Added for index:  180\nTrue result\nFalse result\nResult Added for index:  181\nTrue result\nTrue result\nResult Added for index:  182\nTrue result\nTrue result\nResult Added for index:  183\nTrue result\nTrue result\nResult Added for index:  184\nTrue result\nTrue result\nResult Added for index:  185\nTrue result\nTrue result\nResult Added for index:  186\nTrue result\nTrue result\nResult Added for index:  187\nTrue result\nTrue result\nResult Added for index:  188\nTrue result\nTrue result\nResult Added for index:  189\nTrue result\nTrue result\nResult Added for index:  190\nTrue result\nTrue result\nResult Added for index:  191\nTrue result\nTrue result\nResult Added for index:  192\nTrue result\nTrue result\nResult Added for index:  193\nTrue result\nTrue result\nResult Added for index:  194\nTrue result\nTrue result\nResult Added for index:  195\nTrue result\nTrue result\nResult Added for index:  196\nTrue result\nTrue result\nResult Added for index:  197\nTrue result\nTrue result\nResult Added for index:  198\nTrue result\nTrue result\nResult Added for index:  199\n \n[[True, True], [True, True], [True, True], [True, True], [False, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, False], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [False, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, True], [True, False], [True, True], [True, True]]\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_accuracy(predictions_file):\n    # Read True/False values from the file into a list\n    with open(predictions_file, 'r') as file:\n        predictions = [line.strip() == 'True' for line in file]\n\n    print('Testing Dataset Length: ',len(predictions))\n    \n    # Calculate accuracy\n    total_predictions = len(predictions)\n    correct_predictions = sum(predictions)\n    accuracy = correct_predictions / total_predictions * 100\n\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:42:38.403145Z","iopub.execute_input":"2024-03-06T16:42:38.403467Z","iopub.status.idle":"2024-03-06T16:42:38.412120Z","shell.execute_reply.started":"2024-03-06T16:42:38.403443Z","shell.execute_reply":"2024-03-06T16:42:38.410721Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"Accuracy = calculate_accuracy('/kaggle/working/output.txt')\n\nprint('Accuracy: ',Accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:42:42.924490Z","iopub.execute_input":"2024-03-06T16:42:42.924819Z","iopub.status.idle":"2024-03-06T16:42:42.930654Z","shell.execute_reply.started":"2024-03-06T16:42:42.924784Z","shell.execute_reply":"2024-03-06T16:42:42.929748Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Testing Dataset Length:  470\nAccuracy:  95.95744680851064\n","output_type":"stream"}]}]}